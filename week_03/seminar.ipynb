{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar #2: Levenshtein distance, WER\n",
    "\n",
    "Over the past decade, there has been a multiple improvements in the quality of ASR 📈 . But in what units and how the ASR quality is measured?\n",
    "\n",
    "Word (Character, Phoneme) Error Rate (WER/CER/PER) – are the most popular metrics, which try to approximate how we perceive errors in the speech we hear.\n",
    "\n",
    "![human performance wer](https://awni.github.io/images/speech-recognition/wer.svg)\n",
    "\n",
    "In this seminar, we will try out one of the edit distances: the Levenshtein distance, we'll implement it (naively and more optimally), discuss the issue of computational complexity a little, and then we'll show how this distance can be turned into an ASR system quality metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance\n",
    "\n",
    "Consider the sequence of elements (for example, strings are sequences of characters). In fact, the algorithm easily generalizes to sequences of any elements that can be compared for equality, such as sequences of strings (e.g. list of words).\n",
    "\n",
    "Assume that the following operations can be performed on the sequence:\n",
    "* **insertion**: cat → ca<font color='green'>s</font>t,\n",
    "* **deletion**: ca<font color='red'>s</font>t → cat,\n",
    "* **substitution**: c<font color='blue'>a</font>t → c<font color='blue'>u</font>t,\n",
    "\n",
    "and suppose they have equal *costs*. So these operations are enough to translate an arbitrary sequence into any other one. But how to find the optimal way to do this transformation. And what is the \n",
    "\n",
    "### Algorithm definition\n",
    "\n",
    "Levenshtein distance – the number inserts, deletions, and substitutions are required to get a second sequence from one sequence.\n",
    "\n",
    "$$\n",
    "\\mathrm{L}(a, b) = \n",
    "\\begin{cases}\n",
    "    |a|,& \\text{if } |b| = 0, ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{# second sequence is empty} \\\\\n",
    "    |b|,& \\text{if } |a| = 0, ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{# first sequence is empty} \\\\\n",
    "    \\mathrm{L}(\\mathrm{tail}(a), \\mathrm{tail}(b)),& \\text{if } \\mathrm{head}(a) = \\mathrm{head}(b), ~ ~ \\text{# first elements of two sequencies are equal} \\\\\n",
    "    1 + min \n",
    "    \\begin{cases} \n",
    "        \\mathrm{L}(\\mathrm{tail}(a), b), ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{# deletion from first sequence} \\\\ \n",
    "        \\mathrm{L}(a, \\mathrm{tail}(b)), ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \\text{# insertion into first sequence} \\\\ \n",
    "        \\mathrm{L}(\\mathrm{tail}(a), \\mathrm{tail}(b)); ~ ~ ~ ~ \\text{# substitution}\n",
    "    \\end{cases} & \\text{, otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As you can see, the Levenshtein distance is a metric in the mathematical sense (symmetry, positive certainty, triangle inequality).\n",
    "\n",
    "**Question: what is the complexity of this algorithm?**\n",
    "\n",
    "### Naïve recursive implementation (3 points)\n",
    "\n",
    "Let's try to implement the recursive algorithm described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_naive(a, b):\n",
    "    #############################################\n",
    "    # ...\n",
    "    #############################################\n",
    "\n",
    "def run_tests(fn):\n",
    "    assert fn('kitten', 'sitten') == 1\n",
    "    assert fn('kitten', 'sit') == 4\n",
    "    assert fn('kitten', 'puppy') == 6\n",
    "    assert fn('bcabac', 'cabcab') == 3\n",
    "    \n",
    "run_tests(levenshtein_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wagner–Fischer algorithm (3 points)\n",
    "\n",
    "The complexity of the naive implementation of the Levenshtein distance algorithm is exponential. This is due to the fact that we recalculate the distances for the same suffixes more than onese! \n",
    "\n",
    "This can be avoided if we cache the results of calculations in the form of a matrix of distances between suffixes (more conveniently, prefixes), and fill in this matrix iteratively.\n",
    "\n",
    "The resulting algorithm is named *Wagner–Fischer algorithm** and is an example of a dynamic programming algorithm.\n",
    "\n",
    "Auxiliary function for drawing this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_matrix(matrix, row_names, column_names, path=None, mods=None):\n",
    "    row_names = ['#'] + list(row_names)\n",
    "    column_names = ['#'] + list(column_names)\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    plt.figure(figsize=(len(column_names) / 2, len(row_names) / 2))\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    plt.title(\"Levenshtein prefix distances\")\n",
    "    \n",
    "    r = 0 if max(map(len, row_names + column_names)) < 3 else 45\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.xticks(range(len(column_names)), column_names, fontsize=12, rotation=r)\n",
    "    plt.yticks(range(len(row_names)), row_names, fontsize=12, rotation=r)\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            kwargs = {\n",
    "                'color': \"white\" if matrix[i, j] > matrix.max() / 2 else \"black\",\n",
    "                'horizontalalignment': 'center'\n",
    "            }\n",
    "            plt.text(j, i, \"{:,}\".format(matrix[i, j]), **kwargs)\n",
    "    \n",
    "    if path is not None:\n",
    "        for (i, j), mod in zip(path, mods):\n",
    "            colors = {\n",
    "                'same': '#888888',\n",
    "                'subst': '#0000ff',\n",
    "                'del': '#ff0000',\n",
    "                'insert': '#00ff00'\n",
    "            }\n",
    "            \n",
    "            rect = patches.Rectangle(\n",
    "                (j - 0.45, i - 0.45), 0.9, 0.9, \n",
    "                edgecolor=colors[mod], facecolor='none', linewidth=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_matrix([\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [1, 0, 1, 2, 3],\n",
    "    [2, 1, 0, 1, 2],\n",
    "    [3, 2, 1, 1, 1]\n",
    "], 'cat', 'cast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the `levenshtein_distance_matrix` function, which returns **the distance matrix between the prefixes of the two sequences**. The lower-right element of this matrix is the distance between the prefixes that are equal to the original sequences.\n",
    "\n",
    "It is necessary to fill in this matrix line by line: for a new element of this matrix, it is enough to know only its neighbors to the left, top, and left-top.\n",
    "\n",
    "Also we will prepend an element denoting an **empty prefix** to the sequences – this is done in order to initialize the initial boundary values (initialize the first row and the first column of the matrix with the index values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance_matrix(a: Iterable, b: Iterable) -> np.ndarray:\n",
    "    a = ['#'] + list(a)\n",
    "    b = ['#'] + list(b)\n",
    "    d = np.zeros((len(a), len(b)), dtype=int)\n",
    "\n",
    "    #############################################\n",
    "    # ...\n",
    "    #############################################\n",
    "    \n",
    "    return d\n",
    "\n",
    "def levenshtein_dp(a: Iterable, b: Iterable) -> int:\n",
    "    return levenshtein_distance_matrix(a, b)[-1, -1]\n",
    "\n",
    "# first, second = 'sunday', 'saturday'\n",
    "first, second = 'elephant', 'relevant'\n",
    "plot_matrix(levenshtein_distance_matrix(first, second), first, second)\n",
    "\n",
    "def run_tests(fn):\n",
    "    assert fn('kitten', 'sitten') == 1\n",
    "    assert fn('kitten', 'sit') == 4\n",
    "    assert fn('kitten', 'puppy') == 6\n",
    "    assert fn('bcabac', 'cabcab') == 3\n",
    "\n",
    "    import random\n",
    "    for _ in range(100):\n",
    "        first = \"\".join([random.choice('abc') for _ in range(random.choice(range(3, 10)))])\n",
    "        second = \"\".join([random.choice('abc') for _ in range(random.choice(range(3, 10)))])\n",
    "        assert fn(first, second) == levenshtein_naive(first, second)\n",
    "        \n",
    "# lets check our implementation on random sequences\n",
    "run_tests(levenshtein_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtrace (3 points)\n",
    "\n",
    "To understand what insertions, deletions and substitutions were made on the original sequence, you can do a backtrace on the resulting matrix. \n",
    "\n",
    "Let's consider the first sequence as the original one, abd we will call the deletions and inserts relative to it.\n",
    "\n",
    "**Implement** the `backtrace` function, based on the construction logic `levenshtein_distance_matrix`:\n",
    "* write the path to the variable `path` – the list of the coordinates of the matrix cells that lie on the optimal path through the matrix;\n",
    "* and in the `mods` variable, write down the modifications that we make on the original sequence:\n",
    "    * `same` - leaving the element unchanged\n",
    "    * `subst` - replacing the element\n",
    "    * `del` - deleting the element\n",
    "    * `insert` - inserting the element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix([\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [1, 0, 1, 2, 3],\n",
    "    [2, 1, 0, 1, 2],\n",
    "    [3, 2, 1, 1, 1]\n",
    "],\n",
    "    'cat', 'cast', \n",
    "    [(0, 0), (1, 1), (2, 2), (2, 3), (3, 4)], \n",
    "    ['same', 'same', 'same', 'insert', 'same'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace(d):\n",
    "    path = []\n",
    "    mods = []\n",
    "    \n",
    "    ##########################################\n",
    "    # ...\n",
    "    ##########################################\n",
    "\n",
    "    return path, mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second = 'thursday tea', 'friday beer'\n",
    "\n",
    "path, mods = backtrace(levenshtein_distance_matrix(first, second))\n",
    "plot_matrix(levenshtein_distance_matrix(first, second), first, second, path, mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try applying the Levenshtein distance to a sequence of words, not characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = \"но на самом деле турка не был в деле да и клара краля в то время кралась к ларю пока карл у клары крал кораллы за что клара у карла украла кларнет\".split()\n",
    "second = \"но на самом деле турк не был в бери да и клара кала в то время кралась квари пока карлу у правой коалы за что то клара у карла украла кларнет\".split()\n",
    "\n",
    "path, mods = backtrace(levenshtein_distance_matrix(first, second))\n",
    "plot_matrix(levenshtein_distance_matrix(first, second), first, second, path, mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = (np.array(mods) == 'del').sum()\n",
    "S = (np.array(mods) == 'subst').sum()\n",
    "I = (np.array(mods) == 'insert').sum()\n",
    "print(f\"S: {S}\")\n",
    "print(f\"I: {I}\")\n",
    "print(f\"D: {D}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error rate\n",
    "\n",
    "Suppose we have our reference sequence, relative to which we want to calculate the recognition error.\n",
    "\n",
    "**Why do you think the Levenshtein distance is not suitable for measuring the quality of the ASR system?**\n",
    "\n",
    "Because the number of tokens in sentence can be different. Therefore, we need to normalize the Levenshtein distance by the length of the reference.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Word (character, phoneme) error rate can then be computed as:\n",
    "\n",
    "$$\n",
    "\\mathrm{WER} = \\frac{\\mathrm{S} + \\mathrm{I} + \\mathrm{D}}{\\mathrm{N}}, \\text{where:} \\\\\n",
    "\\text{S is the number of substitutions,} \\\\\n",
    "\\text{I is the number of insertions,} \\\\\n",
    "\\text{D is the number of deletions,} \\\\\n",
    "\\text{N is the length of reference.}\n",
    "$$\n",
    "\n",
    "### WER vs CER vs PER (1 point)\n",
    "\n",
    "Implement the `error_rate` function, which will calculate the prediction error for a given sequence of tokens (words, characters or phonemes) by formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(reference: Iterable, predicted: Iterable) -> float:\n",
    "    ######################################\n",
    "    # ...\n",
    "    ######################################\n",
    "\n",
    "first = \"но на самом деле турка не был в деле да и клара краля в то время кралась к ларю пока карл у клары крал кораллы за что клара у карла украла кларнет\"\n",
    "second = \"но на самом деле турк не был в бери да и клара кала в то время кралась квари пока карлу у правой коалы за что то клара у карла украла кларнет\"\n",
    "print('WER:', np.round(error_rate(first.split(), second.split()), 4))\n",
    "print('CER:', np.round(error_rate(first, second), 4))\n",
    "\n",
    "first = \"n o <space> n ɐ <space> s a+ m ə m <space> dʲ e+ lʲ e <space> t u+ r k ə <space> nʲ e <space> b ɨ+ lˠ <space> f <space> dʲ e+ lʲ e <space> d ɐ <space> ɪ <space> k lˠ a+ r ə <space> k r a+ lʲ ə <space> f <space> t ɐ <space> v rʲ e+ mʲ ə <space> k r a+ lˠ ə sʲ <space> k <space> lˠ ɐ rʲ u+ <space> p ɐ k a+ <space> k a+ r lˠ <space> ʊ <space> k lˠ a+ r ɨ <space> k r a+ lˠ <space> k ɐ r a+ lˠ ɨ <space> z ɐ <space> t͡ɕ t ɐ <space> k lˠ a+ r ə <space> ʊ <space> k a+ r lˠ ə <space> ʊ k r a+ lˠ ə <space> k lˠ ɐ r nʲ e+ t\"\n",
    "second = \"n o <space> n ɐ <space> s a+ m ə m <space> dʲ e+ lʲ e <space> t u+ r k <space> nʲ e <space> b ɨ+ lˠ <space> f <space> bʲ e+ rʲ ɪ <space> d ɐ <space> ɪ <space> k lˠ a+ r ə <space> k a+ lˠ ə <space> f <space> t ɐ <space> v rʲ e+ mʲ ə <space> k r a+ lˠ ə sʲ <space> k v ɐ rʲ i+ <space> p ɐ k a+ <space> k a+ r lˠ ʊ <space> ʊ <space> p r a+ v ə j <space> k ɐ a+ lˠ ɨ <space> z ɐ <space> t͡ɕ t ɐ <space> t ɐ <space> k lˠ a+ r ə <space> ʊ <space> k a+ r lˠ ə <space> ʊ k r a+ lˠ ə <space> k lˠ ɐ r nʲ e+ t\"\n",
    "print('PER:', np.round(error_rate(first.split(), second.split()), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
