{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework # 1: Audio Classification\n",
    "\n",
    "In this work you will master all the basic skills with audio applied to the problem of classification.\n",
    "\n",
    "You will:\n",
    "* 🔥 master `torchaudio` as a library for working with audio in conjunction with `torch`\n",
    "* 🔊 try out the different feature representations of the audio signal in practice\n",
    "* 👨‍🔬 develop an audio classification model based on recurrent networks\n",
    "* 🗣 test the trained model on real data (on your own voice)\n",
    "\n",
    "We will work with the **AudioMNIST** dataset (similar to the MNIST dataset, but from the audio world). It was presented in [paper](https://arxiv.org/pdf/1807.03418.pdf) 📑 , dedicated to the study of audio representations. But this article is indirectly related to our homework.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*vkUOacXAsNIQCpDu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries\n",
    "\n",
    "We will work with `torch` version `1.7.1` (with the corresponding version of `touchaudio`).\n",
    "\n",
    "While the installation is in progress, you can run your eyes through the [documentation](https://pytorch.org/audio/stable/transforms.html) to `torchaudio` – from this library, we will only need a few transforms, as well as audio loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "% pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "% pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining AudioMNIST dataset 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣\n",
    "\n",
    "To get the dataset, clone the repository (`~1Gb`) running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L:bash\n",
    "# If in Yandex Datasphere run as below:\n",
    "git clone https://github.com/soerenab/AudioMNIST\n",
    "# If run locally:\n",
    "# ! git clone https://github.com/soerenab/AudioMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, Audio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this Jupyter Notebook on a computer with a video card (**GPU**), you need to specify which card you want to work with, and you also need to make sure that `torch` sees it. \n",
    "\n",
    "If you *really* want, you can do your homework without using a video card, but then the training will take a very long time.\n",
    "\n",
    "**Tip**: if the video card is an exhausted resource (i.e. you have a usage quota, or you pay for usage), then it is better to **debug** training without the GPU: to make sure that the model does not crash with an error, and the loss decreases in the first iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU found! 🎉')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Only CPU found! 💻')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the set for testing 🎛\n",
    "\n",
    "The set for validation will be taken from the `AudioMNIST` dataset. But as you know, there is a risk of overfitting not only on training set, but also on validation set. Therefore, for the purity of the experiment, we will evaluate the work of our algorithm on real data, namely, our voice!\n",
    "\n",
    "You can record your own (or someone else's) voice speaking out the numbers, so you will have a real mini-dataset for testing. You will need it exclusively. We will test your model on our **own** testing set. 👀\n",
    "\n",
    "To record a voice, you can use web-sites like [this](https://voice-recorder-online.com), and also you can do this via the command line (you need the `sox` utility):\n",
    "\n",
    "```\n",
    "rec recording.wav trim 0 10   # record a 10-second audio fragment\n",
    "of play recording.wav         # listen to it\n",
    "```\n",
    "\n",
    "The recording may differ not only in format (be `mp3`, `ogg`), but in sampling rate (`22050Hz`, `44100Hz`), being a stereo recording, and so on. \n",
    "\n",
    "To bring the record to the unified format we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!S\n",
    "# -> 16kHz, 16bit, mono\n",
    "! sox recording.wav -r 16000 -b 16 -c 1 recording_16kHz.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "x, sr = torchaudio.load('recording_16kHz.wav')\n",
    "x = x[0].numpy()\n",
    "\n",
    "DIGIT_NAMES = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "print(\", \".join(DIGIT_NAMES))\n",
    "display(Audio(x, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to break this record into fragments, each of which would contain audio with the pronunciation of **only one** digit.\n",
    "\n",
    "Let's use the energy ⚡️ VAD!\n",
    "\n",
    "**Calculate** the energy of the signal in a windowed way (choose the window size however you like) and look at its plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "# empirically, you can choose the optimal window size\n",
    "win_size = 1024\n",
    "\n",
    "energies_db = []\n",
    "for i in range(len(x) // win_size + 1):\n",
    "    x_win = x[i * win_size: (i + 1) * win_size]\n",
    "    # calculate energy of signal fragment\n",
    "    ################################################################################\n",
    "    # ...\n",
    "    ################################################################################\n",
    "energies_db = np.array(energies_db)\n",
    "\n",
    "def plot_signal_energies(x, sr, energies_db, intervals=None):\n",
    "    fig = plt.figure(figsize=(len(x) / sr, 2.5))\n",
    "    plt.plot(x, color='#20639b', label='audio signal')\n",
    "    plt.xticks(np.arange(0, len(x), sr), np.int32(np.arange(0, len(x), sr) / sr))\n",
    "    plt.grid(alpha=0.5, linestyle='--')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.ylim([-0.51, 0.51])\n",
    "    plt.xlim(-50, len(x) + 50)\n",
    "    if intervals:\n",
    "        for a, b in intervals:            \n",
    "            plt.gca().add_patch(patches.Rectangle(\n",
    "                (a, -0.51), (b - a), 1.2, \n",
    "                linewidth=1, edgecolor='none', \n",
    "                facecolor='#3caea3', alpha=0.3))\n",
    "\n",
    "    plt.twinx()\n",
    "    plt.plot(\n",
    "        np.linspace(0, len(x), len(energies_db)), energies_db, \n",
    "        color='#3caea3', label='energy (window size = 512)')\n",
    "    plt.ylabel('Energy [dB]')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "plot_signal_energies(x, sr, energies_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you will have no trouble coming up with **a heuristic** for finding the boundaries of spoken words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "# list of digit intervals [(a_0, b_0), ...]\n",
    "# where a_i -- first audio sample of spoken word, b_i -- last sample\n",
    "intervals = []\n",
    "\n",
    "################################################################################\n",
    "# ...\n",
    "################################################################################\n",
    "\n",
    "plot_signal_energies(x, sr, energies_db, intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the original record into fragments and save them to the directory with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "test_directory = 'AudioMNIST/data/test'\n",
    "if not os.path.exists(test_directory):\n",
    "    os.mkdir(test_directory)\n",
    "\n",
    "n = 0\n",
    "for a, b in intervals:\n",
    "    x_digit = x[a: b]\n",
    "    path = f'{test_directory}/{n % 10}_test_{n // 10}.wav'\n",
    "    print(f'\"{DIGIT_NAMES[n % 10]}\" -> {path}')\n",
    "    n += 1\n",
    "    \n",
    "    torchaudio.save(path, torch.FloatTensor(x_digit), sr)\n",
    "    display(Audio(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for the dataset 🗄\n",
    "\n",
    "The class below (the a subclass of `torch.utils.data.Dataset`) holds the logic of loading `AudioMNIST` dataset. All the file handling routins are already written, so you are required to do the following:\n",
    "\n",
    "* 👩‍💻 completed the code (at least, in the `__getitem__` method) for obtaining the audio features (representations) you need from the signal\n",
    "* 🤔 explain your choice with a comment in `__getitem__`\n",
    "\n",
    "You could use the waveform itself as an audio representation (features), **but** as you know from the lecture (*DSP Basics*), this is not the best representation for many tasks.\n",
    "\n",
    "You are able to return to this stage many times while doing you homework, so iterate from simple representations to sophisticated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "class AudioMNISTDataset(data.Dataset):\n",
    "    def __init__(self, directory: str, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "        for filepath in sorted(glob(os.path.join(directory, '*', '?_*_*.wav'))):\n",
    "            digit, speaker, example_id = os.path.basename(filepath).replace('.wav', '').split('_')\n",
    "            add = False\n",
    "            if speaker == 'test' and mode == 'test':\n",
    "                add = True\n",
    "            if speaker != 'test' and int(speaker) % 6 == 0 and mode == 'valid':\n",
    "                add = True\n",
    "            if speaker != 'test' and int(speaker) % 6 != 0 and mode == 'train':\n",
    "                add = True\n",
    "            if add:\n",
    "                self.filepaths.append(filepath)\n",
    "                self.labels.append(int(digit))\n",
    "                \n",
    "        print(f\"Dataset [{mode}]: {len(self.filepaths)} audios\")\n",
    "        \n",
    "        ################################################################################\n",
    "        # ...\n",
    "        ################################################################################\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "        x, sr = torchaudio.load(self.filepaths[idx])\n",
    "        y = torch.LongTensor([self.labels[idx]])[0]\n",
    "\n",
    "        #####################################################################\n",
    "        # ...\n",
    "        #####################################################################\n",
    "        \n",
    "        # ARGUMENTATION:\n",
    "        # I've chosen this representation, because...\n",
    "        \n",
    "        # x: audio features\n",
    "        # y: target digit class\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "train_dataset = AudioMNISTDataset('AudioMNIST/data', mode='train')\n",
    "valid_dataset = AudioMNISTDataset('AudioMNIST/data', mode='valid')\n",
    "test_dataset = AudioMNISTDataset('AudioMNIST/data', mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see what we got...\n",
    "\n",
    "You may see some interesting points in your audio representation, and **may want to try a different representation**. 🕵️‍ Or return to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def show_some_examples(dataset, K, figsize=(10, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for digit in range(10):\n",
    "        indices = np.where(np.array(dataset.labels) == digit)[0]\n",
    "        for i in range(K):\n",
    "            x, y = dataset[indices[i]]\n",
    "            ax = fig.add_subplot(3, 10, digit + i * 10 + 1)\n",
    "            # roate\n",
    "            if i == 0:\n",
    "                ax.set_title(DIGIT_NAMES[digit])\n",
    "            ax.imshow(x)\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"visualize dataset for training\")\n",
    "show_some_examples(train_dataset, 2)\n",
    "\n",
    "print(\"visualize dataset for testing\")\n",
    "show_some_examples(test_dataset, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can determine **which number is pronounced** looking at it? If so, then the computer can!\n",
    "\n",
    "Note that the audios have different durations, at least because the words have different numbers of phonemes. Thus, our feature tensors do not have a fixed size either.\n",
    "\n",
    "```\n",
    "- Can we bring them to the same size?\n",
    "- Yes, we can. And it will even work (see the dataset article).\n",
    "- So why don't we do that?\n",
    "- This approach has a limited scope. And here's why:\n",
    "```\n",
    "\n",
    "* First, in real ASR systems, we do not classify words, but *sounds* (graphemes, phonemes) – they are more difficult to localize and difficult to classify independently. Although, for example, for the task of classifying the sounds of birdsong, this approach would be normal.\n",
    "* Secondly, convolutional neural networks (actually for which we would like images of the same size) poorly orient in space, since they react to patterns, and may not pay attention to their spatial location. In our problem, the words are quite unique, so they are uniquely identified by a set of sounds (phonemes), so this approach would work. But we're taking a course in **speech recognition and synthesis, not sound classification**. 🙂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation class 📦\n",
    "\n",
    "All DL frameworks work with tensors (not lists of tensors). You need to somehow concatenate audio into a batch, but initially they are of different lengths. You can try zero padding (don't forget to store original length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "class Collate:\n",
    "    def __call__(self, batch: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        ################################################################################\n",
    "        # ...\n",
    "        ################################################################################\n",
    "\n",
    "        \n",
    "        # xs: batch of features\n",
    "        # ls: batch of lengths\n",
    "        # ys: batch of targets\n",
    "        return xs, ls, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "collate_fn = Collate()\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, \n",
    "    num_workers=4, drop_last=True, collate_fn=collate_fn)\n",
    "valid_loader = data.DataLoader(\n",
    "    valid_dataset, batch_size=32, shuffle=False, \n",
    "    num_workers=4, collate_fn=collate_fn)\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset, batch_size=len(test_dataset), shuffle=False, \n",
    "    num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "xs, ls, ys = next(iter(train_loader))\n",
    "print(xs.size(), ls.size(), ys.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network classifier 🤖\n",
    "\n",
    "Write a model for classifying audios. It is desirable to use some recurrent layer in the model. Start with a simple one, and make it more complicated if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        ################################################################################\n",
    "        # ...\n",
    "        ################################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        ################################################################################\n",
    "        # ...\n",
    "        ################################################################################\n",
    "        return x\n",
    "    \n",
    "def create_model_and_optimizer(device):\n",
    "    ################################################################################\n",
    "    # ...\n",
    "    ################################################################################\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation, testing 🏋 ️\n",
    "\n",
    "The minimally sufficient code for training and testing is already written. But feel free to modify the code (but try not to break the compatibility).\n",
    "\n",
    "**Tips**:\n",
    "* before you start long-term training, see how your model behaves on the first iterations/epochs\n",
    "* it is desirable that the training epoch takes less one minute; if longer, maybe your model is too complex\n",
    "* models converge quickly, so it hardly makes sense to do training more than 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def train(model, optimizer, loader, metrics):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for x, lengths, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x, lengths)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    metrics['train_loss'].append(np.mean(losses) )\n",
    "\n",
    "\n",
    "def evaluate(model, loader, metrics, mode):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    cm = np.zeros((10, 10), dtype=np.int32)\n",
    "    with torch.no_grad():\n",
    "        valid_losses = []\n",
    "        for x, lengths, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "    \n",
    "            y_pred = model(x, lengths)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            y_pred = y_pred.argmax(axis=-1)\n",
    "            for i in range(len(x)):\n",
    "                cm[y[i].item(), y_pred[i].item()] += 1\n",
    "    \n",
    "    metrics[f'{mode}_loss'].append(np.mean(losses))\n",
    "    metrics[f'{mode}_accuracy'].append(np.trace(cm) / np.sum(cm))\n",
    "    metrics[f'{mode}_confusion'].append(cm)\n",
    "\n",
    "\n",
    "def train_from_scratch(model, optimizer, train_loader, valid_loader, test_loader):\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'valid_loss': [],\n",
    "        'valid_accuracy': [],\n",
    "        'valid_confusion': [],\n",
    "        'test_loss': [],\n",
    "        'test_accuracy': [],\n",
    "        'test_confusion': []\n",
    "    }\n",
    "\n",
    "    best_valid_accuracy = 0.0\n",
    "    for epoch in tqdm(range(20)):\n",
    "        train(model, optimizer, train_loader, metrics)\n",
    "                \n",
    "        evaluate(model, valid_loader, metrics, 'valid')\n",
    "        evaluate(model, test_loader, metrics, 'test')\n",
    "        \n",
    "        if metrics['valid_accuracy'][-1] > best_valid_accuracy:\n",
    "            best_valid_accuracy = metrics['valid_accuracy'][-1]\n",
    "            torch.save({\n",
    "                'state_dict': model.state_dict(), \n",
    "                'metrics': metrics\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "checkpoint_path = 'model.pth'\n",
    "model, optimizer = create_model_and_optimizer(device)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f'Loading model weights from {checkpoint_path}')\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    metrics = ckpt['metrics']\n",
    "    evaluate(model, valid_loader, metrics, 'valid')\n",
    "    evaluate(model, test_loader, metrics, 'test')\n",
    "else:\n",
    "    print('Training model from scratch..')\n",
    "    metrics = train_from_scratch(model, optimizer, train_loader, valid_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the results 📉 📈\n",
    "\n",
    "We will select the model based on the best accuracy value during validation. Let's evaluate the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def plot_accuracies(valid_accuracy, test_accuracy, best_epoch):\n",
    "    best_valid = valid_accuracy[best_epoch]\n",
    "    best_test = test_accuracy[best_epoch]\n",
    "    \n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(valid_accuracy, label='valid', color='#20639b')\n",
    "    plt.plot(test_accuracy, label='test', color='#3caea3')\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.vlines(best_epoch, 0.6, 1, color='#ed553b', linestyle='--', label='best')\n",
    "    plt.hlines(best_valid, 0, best_epoch, linestyle='--', color='#555555')\n",
    "    plt.hlines(best_test, 0, best_epoch, linestyle='--', color='#555555')\n",
    "    plt.plot(best_epoch, best_valid, marker='o', color='#20639b')\n",
    "    plt.plot(best_epoch, best_test, marker='o', color='#3caea3')\n",
    "    plt.text(0, best_valid - 0.02, f\"{best_valid:.4f}\")\n",
    "    plt.text(0, best_test - 0.02, f\"{best_test:.4f}\")\n",
    "    plt.ylim(0.8, 1.01)\n",
    "    plt.xlim(0, len(valid_accuracy) - 1)\n",
    "    plt.xticks(range(len(valid_accuracy)))\n",
    "    plt.yticks(np.linspace(0.8, 1.0, 11))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', normalize=False):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    plt.title(title)\n",
    "\n",
    "    if names is not None:\n",
    "        tick_marks = np.arange(len(names))\n",
    "        plt.xticks(tick_marks, names, rotation=45)\n",
    "        plt.yticks(tick_marks, names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            kwargs = {\n",
    "                'color': \"white\" if cm[i, j] > thresh else \"black\",\n",
    "                'horizontalalignment': 'center'\n",
    "            }\n",
    "            if normalize:\n",
    "                plt.text(j, i, \"{:0.2f}\".format(cm[i, j]), **kwargs)\n",
    "            else:\n",
    "                plt.text(j, i, \"{:,}\".format(cm[i, j]), **kwargs)\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel(f'Predicted label\\naccuracy={accuracy:0.4f}')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# choosing model by best accuracy on validation set\n",
    "best = np.argmax(metrics['valid_accuracy'])\n",
    "best_cm = metrics['test_confusion'][best]\n",
    "\n",
    "plot_accuracies(metrics['valid_accuracy'], metrics['test_accuracy'], best)\n",
    "plot_confusion_matrix(best_cm, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "Augmentations are applied to training data to improve the generalization ability of the deep neural model. Think about what augmentations you can apply to the audio, and then check how they work by experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!S\n",
    "# run experiments here, reuse previously defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions 🧑‍🎓\n",
    "\n",
    "* What challenges did you encounter while completing this task?\n",
    "* What skills have you acquired while doing this task?\n",
    "* How difficult did you find this task (on a scale from 0 to 10), and why?\n",
    "* What did you like in this homework, and what didn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grades criteria\n",
    "\n",
    "```\n",
    "[ ] (1 point)  implement energy VAD for audio splitting\n",
    "[ ] (1 point)  explain the reason for choosing the audio representation features\n",
    "[ ] (3 points) train classification model (> 95% accuracy on validation)\n",
    "[ ] (2 points) achieve good test-score on our side (> 85% accuracy on hidden sample)\n",
    "[ ] (2 points) provide augmentation experiments\n",
    "[ ] (1 point)  write conclusions\n",
    "```\n",
    "\n",
    "The results of this task are two artifacts:\n",
    "1. this Jupiter Notebook (`.ipynb`) with completed cells\n",
    "2. checkpoint of the `model.path` model on the \"best\" iteration\n",
    "\n",
    "Save the artifacts to a directory named `{your last name}_{your first name}_hw1` and pack them in `.zip` archive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
